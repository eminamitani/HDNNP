#!/usr/bin/env python
# -*- coding: utf-8 -*-

from modules import settings as stg

import csv
import shutil
import traceback
import pickle
import sys
import os
from os import path
from skopt import gp_minimize
from skopt.utils import use_named_args
import numpy as np
import chainer
from chainer.training.triggers import EarlyStoppingTrigger
import chainer.training.extensions as ext
import chainermn

from modules.data import DataGenerator
from modules.model import SingleNNP, HDNNP
from modules.updater import HDUpdater
from modules.util import pprint, mkdir, flatten_dict, set_hyperparameter
from modules.util import ChainerSafelyTerminate, dump_lammps, dump_result
from modules.chainer_extensions import Evaluator
from modules.chainer_extensions import set_log_scale
from modules.chainer_extensions import scatter_plot


def main():
    assert stg.args.mode in ['training', 'param_search', 'sym_func', 'prediction', 'phonon']
    mkdir(stg.file.out_dir)

    if stg.args.mode == 'training':
        try:
            main_training()
        except Exception:
            if stg.mpi.rank == 0:
                traceback.print_exc()
        finally:
            shutil.copy('./settings.py', path.join(stg.file.out_dir, 'settings.py'))

    elif stg.args.mode == 'param_search':
        try:
            # bayesian search of hyperparameters
            seed = np.random.get_state()[1][0]
            seed = stg.mpi.comm.bcast(seed, root=0)
            res = gp_minimize(objective_func, stg.skopt.space,
                              n_random_starts=stg.skopt.init_num,
                              n_calls=stg.skopt.max_num,
                              acq_func=stg.skopt.acq_func,
                              random_state=seed,
                              verbose=True,
                              callback=stg.skopt.callback)
            if stg.mpi.rank == 0:
                with open(path.join(stg.file.out_dir, 'skopt_result.csv'), 'w') as f:
                    writer = csv.writer(f, lineterminator='\n')
                    writer.writerow([space.name for space in stg.skopt.space] + ['score'])
                    writer.writerows([x + [fun] for x, fun in zip(res.x_iters, res.func_vals)])

            # main training with best hyperparameters
            # best hyperparameters are stored in 'res.x'
            for space, value in zip(stg.skopt.space, res.x):
                set_hyperparameter(space.name, value)
            main_training()
        except Exception:
            if stg.mpi.rank == 0:
                traceback.print_exc()
        finally:
            shutil.copy('./settings.py', path.join(stg.file.out_dir, 'settings.py'))

    elif stg.args.mode in ['sym_func']:
        stg.dataset.preproc = None
        DataGenerator(stg.dataset.xyz_file, 'xyz')

    elif stg.args.mode == 'prediction':
        _, energy, forces = predict()
        pprint('energy:\n{}'.format(energy.data))
        pprint('forces:\n{}'.format(forces.data))

    elif stg.args.mode == 'phonon':
        name = path.splitext(path.split(stg.args.masters)[1])[0]
        dataset, _, forces = predict()
        phonopy = dataset.phonopy
        phonopy.set_forces(forces.data)
        phonopy.produce_force_constants()

        pprint('drawing phonon band structure ... ', end='')
        phonopy_plt = stg.phonopy.callback(phonopy)
        phonopy_plt.savefig(path.join(stg.file.out_dir, 'ph_band_HDNNP_{}.png'.format(name)))
        phonopy_plt.close()
        pprint('done')

        shutil.copy('./phonopy_settings.py', path.join(stg.file.out_dir, 'phonopy_settings.py'))


def main_training():
    generator = DataGenerator(stg.dataset.xyz_file, 'xyz')
    dataset, elements = generator.holdout(ratio=stg.dataset.ratio)
    masters, result = training(dataset, elements)
    if stg.mpi.rank == 0:
        generator.preproc.save(path.join(stg.file.out_dir, 'preproc.npz'))
        chainer.serializers.save_npz(path.join(stg.file.out_dir, 'masters.npz'), masters)
        dump_lammps(path.join(stg.file.out_dir, 'lammps.nnp'), generator.preproc, masters)
        dump_result(path.join(stg.file.out_dir, 'result.yaml'), result)


@use_named_args(stg.skopt.space)
def objective_func(**params):
    for key, value in params.items():
        set_hyperparameter(key, value)
    results = []
    with open(os.devnull, 'w') as devnull:
        sys.stdout = devnull
        generator = DataGenerator(stg.dataset.xyz_file, 'xyz')
        for i, (dataset, elements) in enumerate(
                generator.cross_validation(ratio=stg.dataset.ratio, kfold=stg.skopt.kfold)):
            _, result = training(dataset, elements, output=False)
            results.append(result['observation'][-1][stg.model.metrics])
        sys.stdout = sys.__stdout__
    return sum(results) / stg.skopt.kfold


def training(dataset, elements, output=True):
    result = {'training_time': 0.0, 'observation': []}

    # model and optimizer
    masters = chainer.ChainList(*[SingleNNP(element) for element in elements])
    master_opt = chainer.optimizers.Adam(stg.model.init_lr)
    master_opt = chainermn.create_multi_node_optimizer(master_opt, stg.mpi.chainer_comm)
    master_opt.setup(masters)
    master_opt.add_hook(chainer.optimizer_hooks.Lasso(stg.model.l1_norm))
    master_opt.add_hook(chainer.optimizer_hooks.WeightDecay(stg.model.l2_norm))

    for train, test, composition in dataset:
        config = train.config

        # iterators
        train_iter = chainer.iterators.SerialIterator(train, stg.dataset.batch_size // stg.mpi.size,
                                                      repeat=True, shuffle=True)
        test_iter = chainer.iterators.SerialIterator(test, stg.dataset.batch_size // stg.mpi.size,
                                                     repeat=False, shuffle=False)

        # model
        hdnnp = HDNNP(composition)
        hdnnp.sync_param_with(masters)
        main_opt = chainer.Optimizer()
        main_opt = chainermn.create_multi_node_optimizer(main_opt, stg.mpi.chainer_comm)
        main_opt.setup(hdnnp)

        # triggers
        interval = (stg.model.interval, 'epoch')
        stop_trigger = EarlyStoppingTrigger(check_trigger=interval, monitor=stg.model.metrics,
                                            patients=stg.model.patients, mode='min', verbose=True,
                                            max_trigger=(stg.model.epoch, 'epoch'))

        # updater and trainer
        updater = HDUpdater(train_iter, optimizer={'main': main_opt, 'master': master_opt})
        trainer = chainer.training.Trainer(updater, stop_trigger, path.join(stg.file.out_dir, config))

        # extensions
        trainer.extend(ext.ExponentialShift('alpha', 1 - stg.model.lr_decay,
                                            target=stg.model.final_lr, optimizer=master_opt))
        evaluator = Evaluator(iterator=test_iter, target=hdnnp)
        trainer.extend(chainermn.create_multi_node_evaluator(evaluator, stg.mpi.chainer_comm))
        if stg.mpi.rank == 0 and output:
            trainer.extend(ext.LogReport(log_name='training.log'))
            trainer.extend(ext.PrintReport(['epoch', 'iteration', 'main/RMSE', 'main/d_RMSE', 'main/tot_RMSE',
                                            'validation/main/RMSE', 'validation/main/d_RMSE',
                                            'validation/main/tot_RMSE']))
            trainer.extend(scatter_plot(hdnnp, test), trigger=interval)
            if stg.args.verbose:
                trainer.extend(ext.PlotReport(['main/tot_RMSE', 'validation/main/tot_RMSE'], 'epoch',
                                              file_name='RMSE.png', marker=None, postprocess=set_log_scale))

        if stg.args.mode == 'training' and stg.args.resume:
            if config != path.split(stg.args.resume)[1]:
                continue
            pprint('Resume training loop.\n\tconfig_type: {}'.format(config))
            trainer_snapshot = path.join(stg.args.resume, 'trainer_snapshot.npz')
            interim_result = path.join(stg.args.resume, 'interim_result.pickle')
            chainer.serializers.load_npz(trainer_snapshot, trainer)
            with open(interim_result, 'rb') as f:
                result = pickle.load(f)
            # clean up
            stg.args.resume = None
            os.remove(trainer_snapshot)
            os.remove(interim_result)

        with ChainerSafelyTerminate(config, trainer, result):
            trainer.run()

    return masters, result


def predict():
    generator = DataGenerator(stg.args.poscar, 'poscar')
    dataset, elements = generator()
    masters = chainer.ChainList(*[SingleNNP(element) for element in elements])
    chainer.serializers.load_npz(stg.args.masters, masters)
    hdnnp = HDNNP(dataset.composition)
    hdnnp.sync_param_with(masters)
    energy, forces = hdnnp.predict(dataset.input, dataset.dinput)
    return dataset, energy, forces


if __name__ == '__main__':
    main()
