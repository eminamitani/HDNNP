#!/usr/bin/env python
# -*- coding: utf-8 -*-

from config import file_
from config import mpi

from os import path
from glob import glob
import sys
import traceback
from datetime import datetime
import argparse
import yaml
from itertools import islice

import hdnnp
import single
from modules.util import mpimkdir, write
from modules.util import DictAsAttributes, HyperParameter


parser = argparse.ArgumentParser('High Dimensional Neural Network Potential')
parser.add_argument('mode', type=str, default='training', choices=['cv', 'training', 'optimize', 'phonon'],
                    help='select mode')
parser.add_argument('hyperparameter_file', nargs='?', type=str, default='hyperparameter.yaml',
                    help='YAML file which contains all of the hyperparameters.')
# parser.add_argument('', type=, default=, help='')
# parser.add_argument('', type=, default=, help='')
# parser.add_argument('', type=, default=, help='')
# parser.add_argument('', type=, default=, help='')
# parser.add_argument('', type=, default=, help='')
parser.add_argument('--test', '-t', action='store_true', default=False,
                    help='perform phonon calculation with trained master models if this flag is set.\
                          master models is specified with option --masters(-m), and structure is read from disp.yaml')
parser.add_argument('--masters', '-m', nargs='*', type=str, default=None,
                    help='trained master models for phonon calculation. this is used with option --test(-t)\
                          default, this is set to "masters.npz" in the latest directory in "file_.out_dir" in "config.py"')
parser.add_argument('--single', '-s', type=str, default=None, choices=['complex', 'sin', 'LJ'],
                    help='function type for SingleNNP. used for debug.')
parser.add_argument('--preconditioning', '-p', type=str, default='pca',
                    help='preconditioning of the input data. defalut is PCA, and threshold is 0.9999')
parser.add_argument('--epoch', '-e', type=int, default=1000,
                    help='# of training epochs.')
parser.add_argument('--batch-size', '-b', type=int, default=100,
                    help='# of data for each minibatches.')
parser.add_argument('--cross-validation', '-cv', type=int, default=None,
                    help='# of split size of dataset("k" of the k-fold CV). perform k-fold CV if this flag is set.')
parser.add_argument('--random-search', type=int, default=None,
                    help='perform hyperparameter random search if this flag is set.\
                          set iteration number to this option. min&max value for each hyperparameter is read from "hyperparameter_file".')
parser.add_argument('--metrics', '-M', type=str, default='validation/main/tot_RMSE',
                    help='metrics of hyperparameter search with CV.')

args = parser.parse_args()
config = DictAsAttributes({'mode': args.mode,
                           'preconditioning': args.preconditioning,
                           'epoch': args.epoch,
                           'batch_size': args.batch_size,
                           'cross_validation': args.cross_validation,
                           'metrics': args.metrics})
with open(args.hyperparameter_file) as f:
    hp_dict = yaml.load(f)
    dataset_hp = DictAsAttributes(hp_dict['dataset'])
    model_hp = HyperParameter(hp_dict['model'], random=args.random_search)
if args.single:
    config.single = args.single
    training = single
else:
    training = hdnnp

if args.test:
    if args.single:
        raise NotImplementedError("option --single(-s) is used for only functional testing.")

    hp = config + dataset_hp + model_hp[0]
    masters_paths = args.masters if args.masters \
        else glob(path.join(file_.out_dir, '*', 'masters.npz'))[-1:]
    poscar = 'test/POSCAR'
    for masters_path in masters_paths:
        hdnnp.test(hp, masters_path, poscar)
else:
    datestr = datetime.now().strftime('%m%d-%H%M%S')
    out_dir = path.join(file_.out_dir, datestr)
    out_dir = mpi.comm.bcast(out_dir, root=0)
    result_file = path.join(out_dir, 'result.yaml')
    mpimkdir(out_dir)

    if args.cross_validation:
        if mpi.rank == 0:
            write(result_file, yaml.dump({'config': dict(config)}, default_flow_style=False))
            write(result_file, yaml.dump({'dataset_hp': dict(dataset_hp)}))
            write(result_file, 'results:\n')
            try:
                for i, set in enumerate(model_hp):
                    worker = i % mpi.size
                    print '# of hyperparameter set: {} (Rank{})'.format(i, worker)
                    hp = config + dataset_hp + set
                    hp.id = i
                    if worker == 0:
                        result = training.run(hp, path.join(out_dir, str(i)))
                    else:
                        result = mpi.comm.recv(source=worker)
                    result.update(dict(set))
                    best = min(best, result, key=lambda r: r[args.metrics]).copy() if 'best' in locals() else result  # NOQA
                    write(result_file, yaml.dump([result], default_flow_style=False))
            except:
                traceback.print_exc()
            finally:
                write(result_file, yaml.dump({'best result': best}, default_flow_style=False))
                print best
                sys.exit()
        else:
            l = len(model_hp)
            for i, set in islice(zip(range(l), model_hp), mpi.rank, l, mpi.size):
                hp = config + dataset_hp + set
                hp.id = i
                result = training.run(hp, path.join(out_dir, str(i)))
                mpi.comm.send(result, dest=0)
    elif mpi.rank == 0:
        write(result_file, yaml.dump({'config': dict(config)}, default_flow_style=False))
        write(result_file, yaml.dump({'dataset_hp': dict(dataset_hp)}))
        hp = config + dataset_hp + model_hp[0]
        result = training.run(hp, out_dir)
        result.update(dict(model_hp[0]))
        write(result_file, yaml.dump({'result': result}, default_flow_style=False))
        print result
