#!/usr/bin/env python
# -*- coding: utf-8 -*-

from config import file_
from config import mpi

from os import path
from datetime import datetime
import argparse
import yaml
from mpi4py import MPI

import hdnnp
import single
from modules.util import mpiprint, mpimkdir, mpiwrite
from modules.util import DictAsAttributes, HyperParameter


def parse_arg():
    parser = argparse.ArgumentParser('High Dimensional Neural Network Potential')
    parser.add_argument('hyperparameter_file', nargs='?', type=str, default='hyperparameter.yaml',
                        help='YAML file which contains all of the hyperparameters.')
    # parser.add_argument('', type=, default=, help='')
    # parser.add_argument('', type=, default=, help='')
    # parser.add_argument('', type=, default=, help='')
    # parser.add_argument('', type=, default=, help='')
    # parser.add_argument('', type=, default=, help='')
    # parser.add_argument('', type=, default=, help='')
    # parser.add_argument('', type=, default=, help='')
    parser.add_argument('--single', '-s', type=str, default=None, choices=['complex', 'sin', 'LJ'],
                        help='function type for SingleNNP. used for debug.')
    parser.add_argument('--preconditioning', '-p', type=str, default='pca',
                        help='preconditioning of the input data. defalut is PCA, and threshold is 0.9999')
    parser.add_argument('--epoch', '-e', type=int, default=1000,
                        help='# of training epochs.')
    parser.add_argument('--batch-size', '-b', type=int, default=100,
                        help='# of data for each minibatches.')
    parser.add_argument('--cross-validation', '-c', type=int, default=None,
                        help='# of split size of dataset("k" of the k-fold CV). perform k-fold CV if this flag is set.')
    parser.add_argument('--metrics', '-m', type=str, default='validation/main/tot_RMSE', help='metrics of hyperparameter search with CV.')
    return parser.parse_args()


args = parse_arg()
with open(args.hyperparameter_file) as f:
    hp_dict = yaml.load(f)
    dataset_hp = DictAsAttributes(hp_dict['dataset'])
    model_hp = HyperParameter(hp_dict['model'])

datestr = datetime.now().strftime('%m%d-%H%M%S')
out_dir = path.join(file_.out_dir, datestr)
out_dir = mpi.comm.bcast(out_dir, root=0)
mpimkdir(out_dir)

results = []
for i, set in enumerate(model_hp):
    if i % mpi.size == mpi.rank:
        hp = dataset_hp + set
        hp.id = i
        hp.preconditioning = args.preconditioning
        hp.epoch = args.epoch
        hp.batch_size = args.batch_size
        hp.cross_validation = args.cross_validation
        if args.single:
            hp.single = args.single
            results.append(single.run(hp, path.join(out_dir, str(hp.id))))
        else:
            results.append(hdnnp.run(hp, path.join(out_dir, str(hp.id))))
results = mpi.comm.reduce(results, root=0, op=MPI.SUM)
mpiprint('\n'.join(map(str, results)))
if mpi.rank == 0:
    best = reduce(lambda x, y: x if x[args.metrics] < y[args.metrics] else y, results)
    mpiwrite(path.join(out_dir, 'result.yaml'), yaml.dump({'best': best, 'all': results}))
